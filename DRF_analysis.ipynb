{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "\n",
    "# Ignore FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Define global variables and paths\n",
    "specific_path = ['L3', 'Mi9', ['T4a', 'T4b', 'T4c', 'T4d'], 'Mi1', 'L1']\n",
    "classification_file_path = './data/classification.txt'\n",
    "connections_file_path = './data/connections.txt'\n",
    "swc_base_path = './data/sk_lod1_783_healed'\n",
    "output_dir = './results/L3_to_T4_to_L1/'\n",
    "\n",
    "# Create output directory if it does not exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Read the data\n",
    "classification = pd.read_csv(classification_file_path)\n",
    "connections = pd.read_csv(connections_file_path)\n",
    "\n",
    "# Create a dictionary mapping cell types to their respective root IDs\n",
    "cell_types = ['Mi1', 'Mi9', 'L1', 'L3', 'T4a', 'T4b', 'T4c', 'T4d', 'Tm3']\n",
    "root_ids_by_type = {cell_type: classification[(classification['cell_type'] == cell_type) & (classification['side'] == 'right')]['root_id'].tolist() for cell_type in cell_types}\n",
    "\n",
    "# Get the list of L3 neuron IDs\n",
    "l3_neuron_ids = root_ids_by_type.get('L3', [])\n",
    "\n",
    "# DFS to find paths\n",
    "def find_paths_dfs(neuron_id, connections, root_ids_by_type):\n",
    "    stack = [(neuron_id, [neuron_id], 0)]  # Stack stores (neuron_id, path, current path index)\n",
    "    paths = set()\n",
    "\n",
    "    while stack:\n",
    "        current_neuron, path, current_path_index = stack.pop()\n",
    "\n",
    "        # Check if the path has reached the target (L1)\n",
    "        if current_path_index >= len(specific_path) - 1:\n",
    "            if current_neuron in root_ids_by_type.get('L1', []):\n",
    "                paths.add(tuple(path))  # Add the valid path\n",
    "            continue\n",
    "\n",
    "        # Get the next cell type(s) in the path\n",
    "        next_cell_type = specific_path[current_path_index + 1]\n",
    "        next_cell_type_set = {root_id for cell_type in (next_cell_type if isinstance(next_cell_type, list) else [next_cell_type]) for root_id in root_ids_by_type.get(cell_type, [])}\n",
    "\n",
    "        # Find connections for the next step in the path\n",
    "        possible_connections = connections[(connections['pre_root_id'] == current_neuron) & (connections['post_root_id'].isin(next_cell_type_set))] if current_path_index < 2 else connections[(connections['post_root_id'] == current_neuron) & (connections['pre_root_id'].isin(next_cell_type_set))]\n",
    "\n",
    "        # Traverse the connections and continue building the path\n",
    "        for _, row in possible_connections.iterrows():\n",
    "            next_neuron = row['post_root_id'] if current_path_index < 2 else row['pre_root_id']\n",
    "            if next_neuron != current_neuron:  # Avoid cycles\n",
    "                new_path = path + [next_neuron]\n",
    "                stack.append((next_neuron, new_path, current_path_index + 1))\n",
    "\n",
    "    return [list(path) for path in paths]  # Return all unique paths\n",
    "\n",
    "# Find all paths starting from the given L3 neurons\n",
    "def find_all_paths(start_neuron_ids, connections, root_ids_by_type):\n",
    "    all_paths = {}\n",
    "    for neuron_id in start_neuron_ids:\n",
    "        if neuron_id in connections['pre_root_id'].values:  # Check if the neuron has outgoing connections\n",
    "            paths = find_paths_dfs(neuron_id, connections, root_ids_by_type)\n",
    "            if paths:\n",
    "                all_paths[neuron_id] = paths  # Store the paths for this neuron\n",
    "    return all_paths\n",
    "\n",
    "# Find all paths for L3 neurons and save the result\n",
    "all_paths = find_all_paths(l3_neuron_ids, connections, root_ids_by_type)\n",
    "all_paths_path = os.path.join(output_dir, 'l1_all_paths.pkl')\n",
    "\n",
    "# Save the paths to a pickle file\n",
    "pd.to_pickle(all_paths, all_paths_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "\n",
    "# Ignore FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Define global variables and paths\n",
    "specific_path = ['L3', 'Tm9', ['T5a', 'T5b', 'T5c', 'T5d'], 'Tm1', 'L2']\n",
    "classification_file_path = './data/classification.txt'\n",
    "connections_file_path = './data/connections.txt'\n",
    "swc_base_path = './data/sk_lod1_783_healed'\n",
    "output_dir = './results/L3_to_T5_to_L2/'\n",
    "\n",
    "# Create output directory if it does not exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Read the data\n",
    "classification = pd.read_csv(classification_file_path)\n",
    "connections = pd.read_csv(connections_file_path)\n",
    "\n",
    "# Create a dictionary mapping cell types to their respective root IDs\n",
    "cell_types = ['Tm1', 'Tm9', 'L2', 'L3', 'T5a', 'T5b', 'T5c', 'T5d', 'Tm3']\n",
    "root_ids_by_type = {cell_type: classification[(classification['cell_type'] == cell_type) & (classification['side'] == 'right')]['root_id'].tolist() for cell_type in cell_types}\n",
    "\n",
    "# Get the list of L3 neuron IDs\n",
    "l3_neuron_ids = root_ids_by_type.get('L3', [])\n",
    "\n",
    "# DFS to find paths\n",
    "def find_paths_dfs(neuron_id, connections, root_ids_by_type):\n",
    "    stack = [(neuron_id, [neuron_id], 0)]  # Stack stores (neuron_id, path, current path index)\n",
    "    paths = set()\n",
    "\n",
    "    while stack:\n",
    "        current_neuron, path, current_path_index = stack.pop()\n",
    "\n",
    "        # Check if the path has reached the target (L2)\n",
    "        if current_path_index >= len(specific_path) - 1:\n",
    "            if current_neuron in root_ids_by_type.get('L2', []):\n",
    "                paths.add(tuple(path))  # Add the valid path\n",
    "            continue\n",
    "\n",
    "        # Get the next cell type(s) in the path\n",
    "        next_cell_type = specific_path[current_path_index + 1]\n",
    "        next_cell_type_set = {root_id for cell_type in (next_cell_type if isinstance(next_cell_type, list) else [next_cell_type]) for root_id in root_ids_by_type.get(cell_type, [])}\n",
    "\n",
    "        # Find connections for the next step in the path\n",
    "        possible_connections = connections[(connections['pre_root_id'] == current_neuron) & (connections['post_root_id'].isin(next_cell_type_set))] if current_path_index < 2 else connections[(connections['post_root_id'] == current_neuron) & (connections['pre_root_id'].isin(next_cell_type_set))]\n",
    "\n",
    "        # Traverse the connections and continue building the path\n",
    "        for _, row in possible_connections.iterrows():\n",
    "            next_neuron = row['post_root_id'] if current_path_index < 2 else row['pre_root_id']\n",
    "            if next_neuron != current_neuron:  # Avoid cycles\n",
    "                new_path = path + [next_neuron]\n",
    "                stack.append((next_neuron, new_path, current_path_index + 1))\n",
    "\n",
    "    return [list(path) for path in paths]  # Return all unique paths\n",
    "\n",
    "# Find all paths starting from the given L3 neurons\n",
    "def find_all_paths(start_neuron_ids, connections, root_ids_by_type):\n",
    "    all_paths = {}\n",
    "    for neuron_id in start_neuron_ids:\n",
    "        if neuron_id in connections['pre_root_id'].values:  # Check if the neuron has outgoing connections\n",
    "            paths = find_paths_dfs(neuron_id, connections, root_ids_by_type)\n",
    "            if paths:\n",
    "                all_paths[neuron_id] = paths  # Store the paths for this neuron\n",
    "    return all_paths\n",
    "\n",
    "# Find all paths for L3 neurons and save the result\n",
    "all_paths = find_all_paths(l3_neuron_ids, connections, root_ids_by_type)\n",
    "all_paths_path = os.path.join(output_dir, 'L2_all_paths.pkl')\n",
    "\n",
    "# Save the paths to a pickle file\n",
    "pd.to_pickle(all_paths, all_paths_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './result/L3_to_T4_to_L1/l1_all_paths.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 154\u001b[0m\n\u001b[0;32m    152\u001b[0m t4_paths_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./result/L3_to_T4_to_L1/l1_all_paths.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    153\u001b[0m t5_paths_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./result/L3_to_T5_to_L2/l2_all_paths.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 154\u001b[0m t4_paths \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt4_paths_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m t5_paths \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(t5_paths_path)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# Calculate directions and distances for T4 and T5 paths\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\neural\\lib\\site-packages\\pandas\\io\\pickle.py:189\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    188\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\neural\\lib\\site-packages\\pandas\\io\\common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './result/L3_to_T4_to_L1/l1_all_paths.pkl'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Ignore FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Set dataset paths and weight paths\n",
    "classification_file_path = './data/classification.txt'\n",
    "swc_base_path = './data/sk_lod1_783_healed'\n",
    "output_dir = './result/combined_distance_distribution/'\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Read the classification data\n",
    "classification = pd.read_csv(classification_file_path)\n",
    "\n",
    "# Define cell types and subtypes\n",
    "cell_types = ['L3', 'L2', 'L1']\n",
    "t4_subtypes = ['T4a', 'T4b', 'T4c', 'T4d']\n",
    "t5_subtypes = ['T5a', 'T5b', 'T5c', 'T5d']\n",
    "\n",
    "# Create a dictionary to store root IDs by cell type\n",
    "root_ids_by_type = {cell_type: classification[(classification['cell_type'] == cell_type) & (classification['side'] == 'right')]['root_id'].tolist() for cell_type in cell_types + t4_subtypes + t5_subtypes}\n",
    "\n",
    "# Read column assignment data for Cartesian coordinates\n",
    "column_assignment_file_path = './data/column_assignment.txt'\n",
    "df = pd.read_csv(column_assignment_file_path)\n",
    "\n",
    "# Function to get Cartesian coordinates (x, y) for a given root_id\n",
    "def get_cartesian_by_root_id(root_id):\n",
    "    result = df[df['root_id'] == root_id]\n",
    "    if not result.empty:\n",
    "        p, q = result[['x', 'y']].values[0]\n",
    "        x = 2 * p + 1 if q % 2 == 1 else 2 * p\n",
    "        y = q / 2\n",
    "        return np.array([x, y])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to create hexagonal markers\n",
    "def create_hexagon_marker(size=1, orientation=0):\n",
    "    angles = np.linspace(0, 2 * np.pi, 6, endpoint=False) + np.radians(orientation)\n",
    "    return np.column_stack([np.cos(angles), np.sin(angles)]) * size\n",
    "\n",
    "# Function to calculate directions and distances for each path\n",
    "def calculate_all_directions(paths, root_ids_by_type, subtypes):\n",
    "    directions_by_subtype = {subtype: [] for subtype in subtypes}\n",
    "    for neuron_id, neuron_paths in tqdm(paths.items(), desc=\"Calculating directions\"):\n",
    "        l3_soma = get_cartesian_by_root_id(neuron_id)\n",
    "        if l3_soma is not None:\n",
    "            for path in neuron_paths:\n",
    "                if len(path) > 2:\n",
    "                    t_type_id = path[2]\n",
    "                    for subtype in subtypes:\n",
    "                        if t_type_id in root_ids_by_type[subtype]:\n",
    "                            target_soma = get_cartesian_by_root_id(path[-1])\n",
    "                            if target_soma is not None:\n",
    "                                direction = target_soma - l3_soma\n",
    "                                distance = np.linalg.norm(direction)\n",
    "                                T_soma = get_cartesian_by_root_id(t_type_id)\n",
    "                                if T_soma is not None and distance > 0:\n",
    "                                    directions_by_subtype[subtype].append((T_soma, direction, distance))\n",
    "    return directions_by_subtype\n",
    "\n",
    "# Function to calculate average distances and update the coordinates dictionary\n",
    "def calculate_average_distances(all_xy_coords, distance_sums, distance_counts):\n",
    "    average_distances = {key: distance_sums[key] / distance_counts[key] for key in distance_sums.keys()}\n",
    "    for key, avg_distance in average_distances.items():\n",
    "        all_xy_coords[key] = avg_distance\n",
    "\n",
    "# Function to plot direction maps, adjusting color saturation inversely with distance\n",
    "def plot_directions(directions_by_subtype, ax, title):\n",
    "    all_xy_coords = {tuple(get_cartesian_by_root_id(root_id)): -1 for root_id in df['root_id'].dropna() if get_cartesian_by_root_id(root_id) is not None}\n",
    "    norm = plt.Normalize(-np.pi, np.pi)  # Normalize angles\n",
    "    cmap = plt.cm.hsv  # Use HSV color map\n",
    "    direction_sums, distance_sums, distance_counts = {}, {}, {}\n",
    "\n",
    "    # Sum the directions and distances for each point\n",
    "    for subtype, directions in directions_by_subtype.items():\n",
    "        for T_soma, direction, distance in directions:\n",
    "            key = tuple(T_soma)\n",
    "            if key not in direction_sums:\n",
    "                direction_sums[key] = np.array(direction)\n",
    "                distance_sums[key] = distance\n",
    "                distance_counts[key] = 1\n",
    "            else:\n",
    "                direction_sums[key] += np.array(direction)\n",
    "                distance_sums[key] += distance\n",
    "                distance_counts[key] += 1\n",
    "\n",
    "    # Calculate the average distances\n",
    "    calculate_average_distances(all_xy_coords, distance_sums, distance_counts)\n",
    "\n",
    "    # Find the maximum and minimum distances for normalization\n",
    "    max_distance = max(v for v in all_xy_coords.values() if v >= 0)\n",
    "    min_distance = min(v for v in all_xy_coords.values() if v >= 0)\n",
    "\n",
    "    # Create hexagon marker\n",
    "    hexagon_marker = create_hexagon_marker(size=1, orientation=0)\n",
    "\n",
    "    # Plot the points, coloring them based on the average distance and direction\n",
    "    for (x, y), distance in all_xy_coords.items():\n",
    "        if distance < 0:\n",
    "            ax.scatter(x, y, color='gray', s=45, marker=hexagon_marker, edgecolor='gray')\n",
    "        else:\n",
    "            normalized_distance = (distance - min_distance) / (max_distance - min_distance)\n",
    "            alpha = 1\n",
    "\n",
    "            summed_direction = direction_sums[(x, y)]\n",
    "            angle = np.arctan2(summed_direction[1], summed_direction[0])\n",
    "            color = cmap(norm(angle))\n",
    "            ax.scatter(x, y, color=color, alpha=alpha, s=45, marker=hexagon_marker, edgecolor=color)\n",
    "\n",
    "    # Set plot title and remove axis labels\n",
    "    ax.set_title(title, fontsize=30, fontname='Times New Roman')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "# Function to plot a color wheel with a square boundary\n",
    "def plot_color_wheel(ax):\n",
    "    theta = np.linspace(-np.pi, np.pi, 500)\n",
    "    r = np.linspace(0, 1, 500)\n",
    "    T, R = np.meshgrid(theta, r)\n",
    "    norm = plt.Normalize(-np.pi, np.pi)  # Normalize angles\n",
    "    cmap = plt.cm.hsv  # Use HSV color map\n",
    "    angle_colors = cmap(norm(T))\n",
    "\n",
    "    # Adjust color saturation\n",
    "    for i in range(angle_colors.shape[0]):\n",
    "        angle_colors[i, :, :-1] = angle_colors[i, :, :-1] * R[i, :, None] + (1 - R[i, :, None])\n",
    "\n",
    "    ax.pcolormesh(T, R, angle_colors, shading='auto')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "# Read the T4 and T5 path data\n",
    "t4_paths_path = './result/L3_to_T4_to_L1/l1_all_paths.pkl'\n",
    "t5_paths_path = './result/L3_to_T5_to_L2/l2_all_paths.pkl'\n",
    "t4_paths = pd.read_pickle(t4_paths_path)\n",
    "t5_paths = pd.read_pickle(t5_paths_path)\n",
    "\n",
    "# Calculate directions and distances for T4 and T5 paths\n",
    "l3_to_l1_dirs_and_dists_t4 = calculate_all_directions(t4_paths, root_ids_by_type, t4_subtypes)\n",
    "l3_to_l2_dirs_and_dists_t5 = calculate_all_directions(t5_paths, root_ids_by_type, t5_subtypes)\n",
    "\n",
    "# Create a 4x9 layout, with the last column for the color wheel\n",
    "fig = plt.figure(figsize=(20, 8))\n",
    "gs = gridspec.GridSpec(4, 9, figure=fig, wspace=0.1, hspace=0.1)\n",
    "\n",
    "# Plot direction maps for T4 subtypes\n",
    "for i, subtype in enumerate(t4_subtypes):\n",
    "    ax = fig.add_subplot(gs[0:2, (2 * i):(2 * i + 2)])\n",
    "    plot_directions({subtype: l3_to_l1_dirs_and_dists_t4[subtype]}, ax, f\"{subtype}\")\n",
    "\n",
    "# Plot direction maps for T5 subtypes\n",
    "for i, subtype in enumerate(t5_subtypes):\n",
    "    ax = fig.add_subplot(gs[2:4, (2 * i):(2 * i + 2)])\n",
    "    plot_directions({subtype: l3_to_l2_dirs_and_dists_t5[subtype]}, ax, f\"{subtype}\")\n",
    "\n",
    "# Plot the color wheel with a square boundary\n",
    "ax_color_wheel = fig.add_subplot(gs[0:4, 8], projection='polar', frame_on=False)\n",
    "plot_color_wheel(ax_color_wheel)\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
